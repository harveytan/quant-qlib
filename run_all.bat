@echo on
call collect_data.bat
call normalize_data.bat
start "" /WAIT python enrich_data.py
start "" /WAIT python -m baselines.run_baselines.py
call dump_to_bin.bat

:: here manually run training
:: call train.bat (train_ensemble.py)

:: after training completed, run evaluation -> see model's IC performance:
:: IC between 0.20â€“0.30 -> exceptional
:: IC greater than 0.30+ -> Institutional Grade alpha
:: code: python diagnostics_long_short.py

:: Rebalance Daily: run top_long_short.py
:: code:python top_long_short.py


:: Backtest:
:: On each day t:
:: - Use features at date t
:: - Predict scores with your model
:: - Build long/short weights (same build_long_short_portfolio)
:: - Realized 1â€‘day PnL â‰ˆ weighted sum of nextâ€‘day return \$ ret\_ 1d\$ 
:: - Repeat over all days, accumulate PnL
:: code: python backtest_long_short.py


:: This is also a self documentation
:: first step as always run collect_data.bat
:: second step is to run normalize_data.bat
:: third step is to run dump_to_bin.bat
:: 
:: Latest train code is in train.bat
:: Path to improving predictive trading

:: Youâ€™ve built the core: train â†’ reload â†’ evaluate.
::   To turn that into â€œvery high profitable returns,â€ shift from model metrics to 
::   a disciplined trading framework that converts predictions into portfolios, measures economics with costs, and iterates toward robustness.

:: Define a clear objective and label horizon
:: â€¢ 	Target: Predict crossâ€‘sectional returns, not absolute prices.
:: â€¢ 	Horizon: Start with 5â€‘day forward returns (less noisy than 1â€‘day), hold for 5 days, rebalance weekly.
:: â€¢ 	Universe: Keep your 40â€‘stock sandbox for speed; expand only after the pipeline is stable.

:: Build a rigorous evaluation loop
:: â€¢ 	Core metrics:
:: â€¢ 	IC and Rank IC: Perâ€‘day mean and stability.
:: â€¢ 	Economics: CAGR, Sharpe, max drawdown, hit rate, turnover.
:: â€¢ 	Costâ€‘adjusted returns: Slippage + commission deducted.
:: â€¢ 	Validation design:
:: â€¢ 	Walkâ€‘forward: Train on period A, validate on B, test on C; roll forward.
:: â€¢ 	Robustness: Try different months, universes, and horizons; check consistency.
:: â€¢ 	Leakage guards: Ensure features donâ€™t use future info; crossâ€‘sectional zâ€‘score normalization is OK, but keep it within each date.

:: From predictions to a tradable portfolio
:: â€¢ 	Ranking: Each day, rank stocks by predicted return.
:: â€¢ 	Weights:
:: â€¢ 	Long-only: Top decile; weights âˆ zâ€‘score of predictions.
:: â€¢ 	Longâ€‘short: Top decile long, bottom decile short; dollarâ€‘neutral or betaâ€‘neutral.
:: â€¢ 	Risk controls:
:: â€¢ 	Position caps: e.g., max 5% per name.
:: â€¢ 	Sector/industry neutralization: Prevent concentration.
:: â€¢ 	Volatility scaling: Reduce exposure on highâ€‘volatility names/days.
:: â€¢ 	Costs: Model perâ€‘trade commission and slippage; apply to turnover each rebalance.

::ğŸ”§ 1. Feature Engineering
::â€¢ 	Lagged features: Add lagged returns, volatility, or rr_ratio to capture momentum or mean-reversion.
::â€¢ 	Sector indicators: Encode sector membership to help the model learn sector-specific behavior.
::â€¢ 	Macro overlays: Include macro signals like interest rates, VIX, or CPI if available â€” even as dummy features.

::ğŸ§  2. Model Enhancements
::â€¢ 	Stacked models: Combine LightGBM with a linear model or neural net for hybrid learning.
::â€¢ 	Interaction terms: Use polynomial features or tree-based interaction constraints to capture nonlinear relationships.
::â€¢ 	Regularization tuning: Try stronger L1/L2 penalties to reduce overfitting and improve generalization.

::ğŸ§ª 3. Label Refinement
::â€¢ 	Smoothed returns: Use exponentially weighted returns to reduce noise.
::â€¢ 	Risk-adjusted labels: Normalize returns by volatility or drawdown to teach the model about risk.
::â€¢ 	Cohort-aware labels: Penalize stocks that consistently underperform their cohort, even if raw return is positive.

::ğŸ“Š 4. Data Augmentation
::â€¢ 	Synthetic instruments: Create pseudo-stocks by bootstrapping features and returns from real ones.
::â€¢ 	Rolling windows: Train on overlapping windows to increase sample size without adding symbols.
::â€¢ 	Dropout masking: Randomly mask features during training to improve robustness.
::ğŸ§­ 5. Diagnostic Extensions
::- Log IC per date: Track rank correlation to validate score quality.
::- Visualize score vs realized return: Confirm that higher scores lead to better outcomes.
::- Track rr_ratio by score bucket: Validate that your model is separating winners from losers.

::ğŸš€ Bonus: Optuna + Early Stopping + Score Filtering
::- Use Optuna to tune not just hyperparameters, but also:
::- Score thresholds for trade inclusion
::- rr_ratio filters for attribution
::- Drawdown guards for survivability


@REM ğŸ“… 2025-09-29 â€” Trade List â€” 10 holding days
@REM   Buys: BABA(+0.7403), GOOG(+0.7037), AMD(+0.4196), RDDT(+0.3979), MSTR(+0.3163), APP(+0.3099), AAPL(+0.3046), PLTR(+0.2726), DVN(+0.2447), NVDA(+0.2308)
@REM   Sells: ABT(-0.2281), MSFT(-0.2445), SSO(-0.2511), VWO(-0.2519), SPUU(-0.2713), MCD(-0.3503), COST(-0.3851), LLY(-0.3864), SPY(-0.4578), ADBE(-0.5822)
@REM     â± 5d â†’ Buy: +1.2% (10), Sell: +2.2% (10), Spread: -1.0%, Vol: 9.41%, R/R: 0.11 âš ï¸
@REM     â± 10d â†’ Buy: -2.0% (10), Sell: +0.6% (10), Spread: -2.5%, Vol: 10.95%, R/R: 0.23 âš ï¸

@REM ğŸ“… 2025-10-13 â€” Trade List â€” 10 holding days
@REM   Buys: BABA(+1.3572), INTC(+0.7305), AVGO(+0.2189), NET(+0.0654), ADBE(-0.0153), NVDA(-0.0480), NFLX(-0.0681), DVN(-0.0682), APP(-0.0955), GOOG(-0.1011)
@REM   Sells: ABT(-0.3521), SPUU(-0.3562), UNH(-0.3688), QQQ(-0.4080), LLY(-0.4147), MSTR(-0.4769), COST(-0.5164), SPY(-0.5392), VWO(-0.5399), RDDT(-0.5823)

@REM ğŸ“… 2025-10-13 â€” Trade List â€” 10 holding days ( on new model by adding new std() params )
@REM   Buys: NET(-0.3123), NVDA(-0.3682)
@REM   Sells: PEP(-0.5929), APP(-0.6003)

@REM ğŸ“… 2025-10-14 â€” Trade List â€” 10 holding days
@REM   Buys: NET(-0.3328), SPY(-0.3754)
@REM   Sells: APP(-0.5891), PEP(-0.6083)